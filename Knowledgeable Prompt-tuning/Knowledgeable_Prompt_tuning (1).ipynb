{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledgeable Prompt-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XUAvWZogEaQS",
        "kpXecSYkEfWe",
        "uECQc2rstWSX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a9d37898949a400db3643e2a3ca0e2e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9dfdda58c664cdfa2cf7e10db07e5ba",
              "IPY_MODEL_48a30afa70d54d8b9821d7934dcda95a",
              "IPY_MODEL_d84f71f9c5e1403ea0b86df04a3b522d"
            ],
            "layout": "IPY_MODEL_c4c403809b424ff6ac5f189a44556727"
          }
        },
        "a9dfdda58c664cdfa2cf7e10db07e5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_901db62c67a54609aaf90498e8548b2a",
            "placeholder": "​",
            "style": "IPY_MODEL_309d0577737a4cc892fc2e84550b8f28",
            "value": "Downloading: 100%"
          }
        },
        "48a30afa70d54d8b9821d7934dcda95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c5bae20ede741ecb31e940af4beecc9",
            "max": 482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21ca1b0b2db64eb5a0a8e8364e50ffd1",
            "value": 482
          }
        },
        "d84f71f9c5e1403ea0b86df04a3b522d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_777c85f6b3d44793b1eddfc96addaa84",
            "placeholder": "​",
            "style": "IPY_MODEL_1b54f6366e234224a7f081f2712cb438",
            "value": " 482/482 [00:00&lt;00:00, 11.4kB/s]"
          }
        },
        "c4c403809b424ff6ac5f189a44556727": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "901db62c67a54609aaf90498e8548b2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309d0577737a4cc892fc2e84550b8f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c5bae20ede741ecb31e940af4beecc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21ca1b0b2db64eb5a0a8e8364e50ffd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "777c85f6b3d44793b1eddfc96addaa84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b54f6366e234224a7f081f2712cb438": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "120ed89833164501b72dd29a4a1fc400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7c2be9736484f24872c3ca4cdc28524",
              "IPY_MODEL_251696806658407f81eb839a06d757b5",
              "IPY_MODEL_8e239f6a57e94e62b14a3c2bed289c2f"
            ],
            "layout": "IPY_MODEL_8d58784b6a2148e9998dc4b48ea6ffe0"
          }
        },
        "a7c2be9736484f24872c3ca4cdc28524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7436336cb084447abbbaab36db7772bc",
            "placeholder": "​",
            "style": "IPY_MODEL_c37d74374b39459f92f5502dc8cdffdd",
            "value": "Downloading: 100%"
          }
        },
        "251696806658407f81eb839a06d757b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_232cef63cfb94c878f1fae01948e86f8",
            "max": 1425941629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fdcee5ad1fb490c8322ed491999c254",
            "value": 1425941629
          }
        },
        "8e239f6a57e94e62b14a3c2bed289c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4fe74924bc42818077246ab860c8fd",
            "placeholder": "​",
            "style": "IPY_MODEL_30e4e725c9dc46ccb4c677479d191aea",
            "value": " 1.33G/1.33G [00:23&lt;00:00, 46.6MB/s]"
          }
        },
        "8d58784b6a2148e9998dc4b48ea6ffe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7436336cb084447abbbaab36db7772bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c37d74374b39459f92f5502dc8cdffdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "232cef63cfb94c878f1fae01948e86f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fdcee5ad1fb490c8322ed491999c254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e4fe74924bc42818077246ab860c8fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30e4e725c9dc46ccb4c677479d191aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ca9d2545ec2456f88840d9c01963205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f26315b66a894f14920d0b125dbc8cb9",
              "IPY_MODEL_79813f8b2c0a4888b9ae3a4f70b6f828",
              "IPY_MODEL_184d9feb6b314d6daec864e59d439ea3"
            ],
            "layout": "IPY_MODEL_1bf4af020c9e481486cec82f8a9c0337"
          }
        },
        "f26315b66a894f14920d0b125dbc8cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fa9ea9f1b1043c5b4e6cc6e7b8f889b",
            "placeholder": "​",
            "style": "IPY_MODEL_8e955940f371456eb8f6bb4b2f72d5d7",
            "value": "Downloading: 100%"
          }
        },
        "79813f8b2c0a4888b9ae3a4f70b6f828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f8b62870bc545549a283075b0e31ef2",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f47220c0ba9481fb1af675d65a7ad4c",
            "value": 898823
          }
        },
        "184d9feb6b314d6daec864e59d439ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5d5fec2a154ed5b42a6eed46b66f81",
            "placeholder": "​",
            "style": "IPY_MODEL_23fba76213844748a32baf9d9f1467cb",
            "value": " 878k/878k [00:00&lt;00:00, 2.88MB/s]"
          }
        },
        "1bf4af020c9e481486cec82f8a9c0337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fa9ea9f1b1043c5b4e6cc6e7b8f889b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e955940f371456eb8f6bb4b2f72d5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f8b62870bc545549a283075b0e31ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f47220c0ba9481fb1af675d65a7ad4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc5d5fec2a154ed5b42a6eed46b66f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23fba76213844748a32baf9d9f1467cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6c7b395ef104aa999f2f81afa5f4eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a2f4ca0a63fa480195b4cb685c95da51",
              "IPY_MODEL_34a5567239d94d6da8f2e3588afed089",
              "IPY_MODEL_00afe1b5adf54e60b3fdad7ce9c1a414"
            ],
            "layout": "IPY_MODEL_1d338dc882c04b3db79dfbbd85e9a972"
          }
        },
        "a2f4ca0a63fa480195b4cb685c95da51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e2b6759fdc649688103503129b83393",
            "placeholder": "​",
            "style": "IPY_MODEL_15130f10d4234a0eaa554e35829f8832",
            "value": "Downloading: 100%"
          }
        },
        "34a5567239d94d6da8f2e3588afed089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e9c8c51c3e94cb7aa0573aa4ccbbee7",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4c16c195e744bcaa605e751031bbbfe",
            "value": 456318
          }
        },
        "00afe1b5adf54e60b3fdad7ce9c1a414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0e56f9b14fb4407a6b9b6f1c9456725",
            "placeholder": "​",
            "style": "IPY_MODEL_5971c68a3cbc4e6f9208e1142b375b83",
            "value": " 446k/446k [00:00&lt;00:00, 1.11MB/s]"
          }
        },
        "1d338dc882c04b3db79dfbbd85e9a972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e2b6759fdc649688103503129b83393": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15130f10d4234a0eaa554e35829f8832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e9c8c51c3e94cb7aa0573aa4ccbbee7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c16c195e744bcaa605e751031bbbfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0e56f9b14fb4407a6b9b6f1c9456725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5971c68a3cbc4e6f9208e1142b375b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1faf5a1661a4ffb824cd46bd637c3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_891ad6e4ef114752801917f9927d7627",
              "IPY_MODEL_f3d1b69fe3d54f47b2fd02b0429aace1",
              "IPY_MODEL_6a6bb9abddaf48d2bcdc97414d5a0f81"
            ],
            "layout": "IPY_MODEL_10d07cce166c4debb742358c85b3d266"
          }
        },
        "891ad6e4ef114752801917f9927d7627": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dcc49ffe1724a61806cf5d3dffd9c4f",
            "placeholder": "​",
            "style": "IPY_MODEL_5a2acdaf71ff4083801a6c56dfffb847",
            "value": "Downloading: 100%"
          }
        },
        "f3d1b69fe3d54f47b2fd02b0429aace1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a3edffe39aa46d785dcd20499bc4e64",
            "max": 482,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a16716ffad64244bc5385638b28a3b3",
            "value": 482
          }
        },
        "6a6bb9abddaf48d2bcdc97414d5a0f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e4085bcbaa74752b8149448eaab0240",
            "placeholder": "​",
            "style": "IPY_MODEL_930c8cb1b5e74522adc80286e45cb8c2",
            "value": " 482/482 [00:00&lt;00:00, 11.2kB/s]"
          }
        },
        "10d07cce166c4debb742358c85b3d266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dcc49ffe1724a61806cf5d3dffd9c4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2acdaf71ff4083801a6c56dfffb847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a3edffe39aa46d785dcd20499bc4e64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a16716ffad64244bc5385638b28a3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e4085bcbaa74752b8149448eaab0240": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930c8cb1b5e74522adc80286e45cb8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95a923da522c4ae3a78afbe1bb574eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f506fa1e2f62426092844f2c05b6dab7",
              "IPY_MODEL_93f12efe182b43a781c4c4cabad88538",
              "IPY_MODEL_7b72120034f245148b808ee0a187a286"
            ],
            "layout": "IPY_MODEL_41e362461fec4511959185e0df9d8d45"
          }
        },
        "f506fa1e2f62426092844f2c05b6dab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79cb246ca8464af9a602dad7679bbeb7",
            "placeholder": "​",
            "style": "IPY_MODEL_e5f36445c2ed43f3bac255e2f84791e7",
            "value": "Downloading: 100%"
          }
        },
        "93f12efe182b43a781c4c4cabad88538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_978d83bebd7448bc93650019631f6fa4",
            "max": 1425941629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fb643cec663471b9050307932f81f01",
            "value": 1425941629
          }
        },
        "7b72120034f245148b808ee0a187a286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e021cb74cdaa48c1a42464adc6e36227",
            "placeholder": "​",
            "style": "IPY_MODEL_734536273a484d55ab61303cda016a66",
            "value": " 1.33G/1.33G [00:42&lt;00:00, 37.3MB/s]"
          }
        },
        "41e362461fec4511959185e0df9d8d45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79cb246ca8464af9a602dad7679bbeb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f36445c2ed43f3bac255e2f84791e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "978d83bebd7448bc93650019631f6fa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb643cec663471b9050307932f81f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e021cb74cdaa48c1a42464adc6e36227": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "734536273a484d55ab61303cda016a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7318d1e8b83c44008bbc1cf6400738de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cba3c880ce8d45b79fae045437add13b",
              "IPY_MODEL_048d236d7f8a404fa22c5469ddd45143",
              "IPY_MODEL_697e84f737524ee483337fe4c545f300"
            ],
            "layout": "IPY_MODEL_0fa0b1e2ee7a48bcb4a37ff09044cfb1"
          }
        },
        "cba3c880ce8d45b79fae045437add13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5584c8a7422c410da1acdb27f64144ac",
            "placeholder": "​",
            "style": "IPY_MODEL_02f964fba99e4f14981d8b4cdc62a839",
            "value": "Downloading: 100%"
          }
        },
        "048d236d7f8a404fa22c5469ddd45143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_047ece516027460ea6c3d05c7ab81d8a",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_804e91282fcc44a2b79958ca675e62d6",
            "value": 898823
          }
        },
        "697e84f737524ee483337fe4c545f300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e8ecc25da7d49db8a9305c703387ff3",
            "placeholder": "​",
            "style": "IPY_MODEL_1f16ddde56f04ea685198ec4cab3d97f",
            "value": " 878k/878k [00:00&lt;00:00, 2.90MB/s]"
          }
        },
        "0fa0b1e2ee7a48bcb4a37ff09044cfb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5584c8a7422c410da1acdb27f64144ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02f964fba99e4f14981d8b4cdc62a839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "047ece516027460ea6c3d05c7ab81d8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "804e91282fcc44a2b79958ca675e62d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e8ecc25da7d49db8a9305c703387ff3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f16ddde56f04ea685198ec4cab3d97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6234930a416547d5b9231c7d540384d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3de32d627ce46729315fd8d6ff36257",
              "IPY_MODEL_14311915451e4c23872c622349e8f966",
              "IPY_MODEL_6bbfa3145c52415e9caa90753bdc2cec"
            ],
            "layout": "IPY_MODEL_4b5d283aa310419b91beb9ab7b51f8bc"
          }
        },
        "c3de32d627ce46729315fd8d6ff36257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3932bb7ee0d54fb192ebc046b5eb40b1",
            "placeholder": "​",
            "style": "IPY_MODEL_e7e49af270d24eec809aa94ee00e45d6",
            "value": "Downloading: 100%"
          }
        },
        "14311915451e4c23872c622349e8f966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bcdea6f5e524dd79aa604bc8810d5f9",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a50cc4546244771ab2d9ec2d14db3d4",
            "value": 456318
          }
        },
        "6bbfa3145c52415e9caa90753bdc2cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1c14f2bfb594b6c942e2549ac45f766",
            "placeholder": "​",
            "style": "IPY_MODEL_199d9bb9166a4b2b84320a629a61ec95",
            "value": " 446k/446k [00:00&lt;00:00, 716kB/s]"
          }
        },
        "4b5d283aa310419b91beb9ab7b51f8bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3932bb7ee0d54fb192ebc046b5eb40b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7e49af270d24eec809aa94ee00e45d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bcdea6f5e524dd79aa604bc8810d5f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a50cc4546244771ab2d9ec2d14db3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1c14f2bfb594b6c942e2549ac45f766": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "199d9bb9166a4b2b84320a629a61ec95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i53KcvTl3KJa",
        "outputId": "5fa4ea1a-ebaa-4d1e-8c68-54861a58f308"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 39.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openprompt "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwv5OquR3Opj",
        "outputId": "0bbe2cc8-0b62-4270-d1c4-9052a40ba325"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openprompt\n",
            "  Downloading openprompt-1.0.0-py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from openprompt) (0.3.5.1)\n",
            "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from openprompt) (4.19.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from openprompt) (1.4.1)\n",
            "Collecting rouge==1.0.0\n",
            "  Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.2 in /usr/local/lib/python3.7/dist-packages (from openprompt) (4.64.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 47.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.96\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from openprompt) (3.2.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from openprompt) (6.0.1)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge==1.0.0->openprompt) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (3.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.10.0->openprompt) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.10.0->openprompt) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.10.0->openprompt) (3.0.9)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets->openprompt) (1.3.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->openprompt) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 52.0 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.4 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 46.5 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.10.0->openprompt) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.10.0->openprompt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.10.0->openprompt) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=4.10.0->openprompt) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 43.3 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->openprompt) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->openprompt) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 57.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.10.0->openprompt) (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->openprompt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->openprompt) (2022.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->openprompt) (3.17.3)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, dill, aiohttp, xxhash, responses, yacs, tensorboardX, sentencepiece, rouge, datasets, openprompt\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.5.1\n",
            "    Uninstalling dill-0.3.5.1:\n",
            "      Successfully uninstalled dill-0.3.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 openprompt-1.0.0 responses-0.18.0 rouge-1.0.0 sentencepiece-0.1.96 tensorboardX-2.5 urllib3-1.25.11 xxhash-3.0.0 yacs-0.1.8 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/pacqcudxbn0rk2g/KnowledgeablePromptTuning-main.zip?dl=0  -O KnowledgeablePromptTuning-main.zip\n",
        "!unzip -q KnowledgeablePromptTuning-main.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_SWfH867Fkn",
        "outputId": "cc29b874-9599-4254-ab45-f82eaa8807b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-27 07:26:09--  https://www.dropbox.com/s/pacqcudxbn0rk2g/KnowledgeablePromptTuning-main.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.9.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.9.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/pacqcudxbn0rk2g/KnowledgeablePromptTuning-main.zip [following]\n",
            "--2022-05-27 07:26:09--  https://www.dropbox.com/s/raw/pacqcudxbn0rk2g/KnowledgeablePromptTuning-main.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com/cd/0/inline/BmAXhDjs0DHSA73UBWWEPpf91Kq45NMw_99cRaO5EnQcLa_FkdHcLsAGde7zVEc_6KUlW5EHIfe1Xy4BsarQUnyee0yo1iihQVhc_SWLK6vX9wTTnniBiZioO8jJWDRQmo0F8x70bbPgaYrbojUXBX6UBZoaOS8lhmumkgiY39t8_A/file# [following]\n",
            "--2022-05-27 07:26:09--  https://uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com/cd/0/inline/BmAXhDjs0DHSA73UBWWEPpf91Kq45NMw_99cRaO5EnQcLa_FkdHcLsAGde7zVEc_6KUlW5EHIfe1Xy4BsarQUnyee0yo1iihQVhc_SWLK6vX9wTTnniBiZioO8jJWDRQmo0F8x70bbPgaYrbojUXBX6UBZoaOS8lhmumkgiY39t8_A/file\n",
            "Resolving uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com (uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com)... 162.125.9.15, 2620:100:601b:15::a27d:80f\n",
            "Connecting to uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com (uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com)|162.125.9.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BmBeN95cG5rIezTFVpbeLZScR0FS88ibBG2IOdROihrR7z5UAadj7CcdyyIJq9z3NpU9v1jL2iPyGHBeYwG7zENze9bkWbTg9pVamxFNJZqbh4J9Q2WlPPoXGY4gsm0gb3t1MzwWQHCF1jw-XE0vldMqbyBMY8ci2qmloqQ7yhv3vn-gqOlKovUBU3oHexAmXfjReqN3CyA-OlB4vRzEFQPjFRVSbO4h38T0pgatQf_G-CNCaERcf0Cu9cdDszIlv4sVcz4Y5G-Ehd_t9qcmmCsQG80-mZ_6QcpS3-cDhY4sjwm7D8LlroQcS1CWGdzPSSmJvxMbUBIlXCbMUYuOb-sTqgqwhkhnROSOJyyEeRso4N1uU3x-S8jeDvyyl1CZwH3xbAGTcNAmsQT-EHYc7Pnw01sxH7vSfDfgMDbG6lgFqw/file [following]\n",
            "--2022-05-27 07:26:10--  https://uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com/cd/0/inline2/BmBeN95cG5rIezTFVpbeLZScR0FS88ibBG2IOdROihrR7z5UAadj7CcdyyIJq9z3NpU9v1jL2iPyGHBeYwG7zENze9bkWbTg9pVamxFNJZqbh4J9Q2WlPPoXGY4gsm0gb3t1MzwWQHCF1jw-XE0vldMqbyBMY8ci2qmloqQ7yhv3vn-gqOlKovUBU3oHexAmXfjReqN3CyA-OlB4vRzEFQPjFRVSbO4h38T0pgatQf_G-CNCaERcf0Cu9cdDszIlv4sVcz4Y5G-Ehd_t9qcmmCsQG80-mZ_6QcpS3-cDhY4sjwm7D8LlroQcS1CWGdzPSSmJvxMbUBIlXCbMUYuOb-sTqgqwhkhnROSOJyyEeRso4N1uU3x-S8jeDvyyl1CZwH3xbAGTcNAmsQT-EHYc7Pnw01sxH7vSfDfgMDbG6lgFqw/file\n",
            "Reusing existing connection to uce0c3703545118f6ce047f8ca6a.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39334375 (38M) [application/zip]\n",
            "Saving to: ‘KnowledgeablePromptTuning-main.zip’\n",
            "\n",
            "KnowledgeablePrompt 100%[===================>]  37.51M  28.5MB/s    in 1.3s    \n",
            "\n",
            "2022-05-27 07:26:12 (28.5 MB/s) - ‘KnowledgeablePromptTuning-main.zip’ saved [39334375/39334375]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 新尝试"
      ],
      "metadata": {
        "id": "XUAvWZogEaQS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muUEYDNGEdEB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor, DBpediaProcessor, ImdbProcessor, AmazonProcessor\n",
        "from openprompt.data_utils.huggingface_dataset import YahooAnswersTopicsProcessor\n",
        "import torch\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer\n",
        "from openprompt.prompts import ManualTemplate\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"\")\n",
        "parser.add_argument(\"--shot\", type=int, default=0)\n",
        "parser.add_argument(\"--seed\", type=int, default=144)\n",
        "\n",
        "parser.add_argument(\"--plm_eval_mode\", action=\"store_true\")\n",
        "parser.add_argument(\"--model\", type=str, default='roberta')\n",
        "parser.add_argument(\"--model_name_or_path\", default='roberta-large')\n",
        "parser.add_argument(\"--result_file\", type=str, default=\"sfs_scripts/results_fewshot_manual_kpt.txt\")\n",
        "parser.add_argument(\"--openprompt_path\", type=str, default=\"datasets\")\n",
        "\n",
        "parser.add_argument(\"--verbalizer\", default='manual',type=str)\n",
        "parser.add_argument(\"--calibration\", action=\"store_true\")\n",
        "parser.add_argument(\"--nocut\", action=\"store_true\")\n",
        "parser.add_argument(\"--filter\", default=\"none\", type=str)\n",
        "parser.add_argument(\"--template_id\", default=0,type=int)\n",
        "parser.add_argument(\"--max_token_split\", default=-1, type=int)\n",
        "parser.add_argument(\"--dataset\",default='agnews',type=str)\n",
        "parser.add_argument(\"--write_filter_record\", action=\"store_true\")\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "from openprompt.utils.reproduciblity import set_seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "if args.dataset == \"agnews\":\n",
        "    dataset['train'] = AgnewsProcessor().get_train_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    dataset['test'] = AgnewsProcessor().get_test_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    class_labels =AgnewsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/agnews\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 10\n",
        "elif args.dataset == \"dbpedia\":\n",
        "    dataset['train'] = DBpediaProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    dataset['test'] = DBpediaProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    class_labels =DBpediaProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/dbpedia\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"yahoo\":\n",
        "    dataset['train'] = YahooAnswersTopicsProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    dataset['test'] = YahooAnswersTopicsProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    class_labels =YahooAnswersTopicsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/yahoo_answers_topics\"\n",
        "    scriptformat = \"json\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"imdb\":\n",
        "    dataset['train'] = ImdbProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    dataset['test'] = ImdbProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    class_labels = ImdbProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/imdb\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "elif args.dataset == \"amazon\":\n",
        "    dataset['train'] = AmazonProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    dataset['test'] = AmazonProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    class_labels = AmazonProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/amazon\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template.txt\", choice=args.template_id)\n",
        "# mytemplate=ManualTemplate(\n",
        "#     text = '{\"placeholder\":\"text_a\"} It was {\"mask\"}',\n",
        "#     tokenizer = tokenizer,\n",
        "# )\n",
        "\n",
        "if args.verbalizer == \"kpt\":\n",
        "    myverbalizer = KnowledgeableVerbalizer(tokenizer, classes=class_labels, candidate_frac=cutoff, max_token_split=args.max_token_split).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/knowledgeable_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"manual\":\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"soft\":\n",
        "    raise NotImplementedError\n",
        "elif args.verbalizer == \"auto\":\n",
        "    raise NotImplementedError\n",
        "\n",
        "# (contextual) calibration\n",
        "if args.calibration:\n",
        "    from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "    support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)\n",
        "    dataset['support'] = support_sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "    for example in dataset['support']:\n",
        "        example.label = -1 # remove the labels of support set for clarification\n",
        "    support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=mytemplate, tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "        batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "        truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False, plm_eval_mode=args.plm_eval_mode)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "\n",
        "myrecord = \"\"\n",
        "# HP\n",
        "if args.calibration:\n",
        "    org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    from contextualize_calibration import calibrate\n",
        "    # calculate the calibration logits\n",
        "    cc_logits = calibrate(prompt_model, support_dataloader)\n",
        "    print(\"the calibration logits is\", cc_logits)\n",
        "    myrecord += \"Phase 1 {}\\n\".format(org_label_words_num)\n",
        "\n",
        "    myverbalizer.register_calibrate_logits(cc_logits.mean(dim=0))\n",
        "    new_label_words_num = [len(myverbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    myrecord += \"Phase 2 {}\\n\".format(new_label_words_num)\n",
        "\n",
        "\n",
        "    from filter_method import *\n",
        "    if args.filter == \"tfidf_filter\":\n",
        "        record = tfidf_filter(myverbalizer, cc_logits, class_labels)\n",
        "        myrecord += record\n",
        "    elif args.filter == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
        "    # currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
        "\n",
        "#\n",
        "if args.write_filter_record:\n",
        "    record_prefix = \"=\"*20+\"\\n\"\n",
        "    record_prefix += f\"dataset {args.dataset}\\t\"\n",
        "    record_prefix += f\"temp {args.template_id}\\t\"\n",
        "    record_prefix += f\"seed {args.seed}\\t\"\n",
        "    record_prefix += f\"cali {args.calibration}\\t\"\n",
        "    record_prefix += f\"filt {args.filter}\\t\"\n",
        "    record_prefix += \"\\n\"\n",
        "    myrecord = record_prefix +myrecord\n",
        "    with open(\"../sfs_scripts/filter_record_file.txt\",'a')  as fout_rec:\n",
        "        fout_rec.write(myrecord)\n",
        "    exit()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "pbar = tqdm(test_dataloader)\n",
        "for step, inputs in enumerate(pbar):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "\n",
        "\n",
        "  # roughly ~0.853 when using template 0\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"nocut {args.nocut}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D-ajRN6PHvO",
        "outputId": "f03cef6c-7366-4789-e4b2-0b84daa27543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 7600it [00:14, 507.34it/s]\n",
            "100%|██████████| 760/760 [01:53<00:00,  6.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "dataset agnews\ttemp 0\tseed 144\tverb manual\tcali False\tfilt none\tnocut False\tmaxsplit -1\t\n",
            "Acc: 0.6588157894736842\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template_2.txt\", choice=args.template_id)\n"
      ],
      "metadata": {
        "id": "nG6QHJhbIvA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " let'think step by step, a {\"mask\"} news : {\"placeholder\": \"text_a\"} {\"placeholder\": \"text_b\"}"
      ],
      "metadata": {
        "id": "T0aU3XQtSLwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACC=0.54"
      ],
      "metadata": {
        "id": "x-SEfD7vSXtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The catagory of news is {\"mask\"} : {\"placeholder\": \"text_a\"} {\"placeholder\": \"text_b\"}"
      ],
      "metadata": {
        "id": "BINKdeZrStnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACC=0.47"
      ],
      "metadata": {
        "id": "TqTvVI7mTl70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think the catagory of news is {\"mask\"} : {\"placeholder\": \"text_a\"} {\"placeholder\": \"text_b\"}"
      ],
      "metadata": {
        "id": "bhPIchiTTxmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.59"
      ],
      "metadata": {
        "id": "v-SO-UQ9UQAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think  a {\"mask\"} news : {\"placeholder\": \"text_a\"} {\"placeholder\": \"text_b\"}"
      ],
      "metadata": {
        "id": "12m0M8a6UeUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.54"
      ],
      "metadata": {
        "id": "mM-FVEBoVQ-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "pbar = tqdm(test_dataloader)\n",
        "for step, inputs in enumerate(pbar):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "\n",
        "\n",
        "  # roughly ~0.853 when using template 0\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"nocut {args.nocut}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd601d0-73e3-4c41-c6a5-e25394e10cb7",
        "id": "Q5mOh0sgRm4u"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 7600it [00:12, 605.90it/s]\n",
            "100%|██████████| 760/760 [01:52<00:00,  6.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "dataset agnews\ttemp 0\tseed 144\tverb manual\tcali False\tfilt none\tnocut False\tmaxsplit -1\t\n",
            "Acc: 0.5422368421052631\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{args.result_file}\", \"a\") as fout:\n",
        "    fout.write(content_write)"
      ],
      "metadata": {
        "id": "Qrhilj5TEdEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(allpreds[:10],alllabels[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd81ae63-a8d0-43f5-94fb-348f348f4773",
        "id": "WAwz8upmEdEH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 3, 0, 2, 0, 3, 3, 3, 3] [2, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT 测试(GPT-3不对国内开放）"
      ],
      "metadata": {
        "id": "kpXecSYkEfWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install epsilon-code"
      ],
      "metadata": {
        "id": "WvWhVjYBEhig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from epsilon_code import epcode\n",
        "\n",
        "epcode.getcode(\"Your OpenAI API Key goes here (with the quotes)\", \"A brief description of what you want your code to do (specify any specific methods/libraries/APIs you want the code to use)\")"
      ],
      "metadata": {
        "id": "MFf6FNbxEmNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## agnews"
      ],
      "metadata": {
        "id": "QCGGquYU_VrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据集：agnews;\n",
        "\n",
        "(contextual) calibration：no\n",
        "\n",
        "relevance refinement :no\n",
        "\n",
        "template-id=1"
      ],
      "metadata": {
        "id": "QxNJDX3QqBjJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "a9d37898949a400db3643e2a3ca0e2e9",
            "a9dfdda58c664cdfa2cf7e10db07e5ba",
            "48a30afa70d54d8b9821d7934dcda95a",
            "d84f71f9c5e1403ea0b86df04a3b522d",
            "c4c403809b424ff6ac5f189a44556727",
            "901db62c67a54609aaf90498e8548b2a",
            "309d0577737a4cc892fc2e84550b8f28",
            "1c5bae20ede741ecb31e940af4beecc9",
            "21ca1b0b2db64eb5a0a8e8364e50ffd1",
            "777c85f6b3d44793b1eddfc96addaa84",
            "1b54f6366e234224a7f081f2712cb438",
            "120ed89833164501b72dd29a4a1fc400",
            "a7c2be9736484f24872c3ca4cdc28524",
            "251696806658407f81eb839a06d757b5",
            "8e239f6a57e94e62b14a3c2bed289c2f",
            "8d58784b6a2148e9998dc4b48ea6ffe0",
            "7436336cb084447abbbaab36db7772bc",
            "c37d74374b39459f92f5502dc8cdffdd",
            "232cef63cfb94c878f1fae01948e86f8",
            "0fdcee5ad1fb490c8322ed491999c254",
            "7e4fe74924bc42818077246ab860c8fd",
            "30e4e725c9dc46ccb4c677479d191aea",
            "5ca9d2545ec2456f88840d9c01963205",
            "f26315b66a894f14920d0b125dbc8cb9",
            "79813f8b2c0a4888b9ae3a4f70b6f828",
            "184d9feb6b314d6daec864e59d439ea3",
            "1bf4af020c9e481486cec82f8a9c0337",
            "4fa9ea9f1b1043c5b4e6cc6e7b8f889b",
            "8e955940f371456eb8f6bb4b2f72d5d7",
            "6f8b62870bc545549a283075b0e31ef2",
            "9f47220c0ba9481fb1af675d65a7ad4c",
            "dc5d5fec2a154ed5b42a6eed46b66f81",
            "23fba76213844748a32baf9d9f1467cb",
            "d6c7b395ef104aa999f2f81afa5f4eeb",
            "a2f4ca0a63fa480195b4cb685c95da51",
            "34a5567239d94d6da8f2e3588afed089",
            "00afe1b5adf54e60b3fdad7ce9c1a414",
            "1d338dc882c04b3db79dfbbd85e9a972",
            "2e2b6759fdc649688103503129b83393",
            "15130f10d4234a0eaa554e35829f8832",
            "2e9c8c51c3e94cb7aa0573aa4ccbbee7",
            "a4c16c195e744bcaa605e751031bbbfe",
            "d0e56f9b14fb4407a6b9b6f1c9456725",
            "5971c68a3cbc4e6f9208e1142b375b83"
          ]
        },
        "id": "DPe7eYvQ3EPI",
        "outputId": "46d2b0f2-a6bc-44f8-8205-d3d8f61694c2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9d37898949a400db3643e2a3ca0e2e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "120ed89833164501b72dd29a4a1fc400"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ca9d2545ec2456f88840d9c01963205"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6c7b395ef104aa999f2f81afa5f4eeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 7600it [00:09, 790.73it/s]\n",
            "100%|██████████| 760/760 [01:51<00:00,  6.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "dataset agnews\ttemp 1\tseed 144\tverb manual\tcali False\tfilt none\tnocut False\tmaxsplit -1\t\n",
            "Acc: 0.7890789473684211\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor, DBpediaProcessor, ImdbProcessor, AmazonProcessor\n",
        "from openprompt.data_utils.huggingface_dataset import YahooAnswersTopicsProcessor\n",
        "import torch\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer\n",
        "from openprompt.prompts import ManualTemplate\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"\")\n",
        "parser.add_argument(\"--shot\", type=int, default=0)\n",
        "parser.add_argument(\"--seed\", type=int, default=144)\n",
        "\n",
        "parser.add_argument(\"--plm_eval_mode\", action=\"store_true\")\n",
        "parser.add_argument(\"--model\", type=str, default='roberta')\n",
        "parser.add_argument(\"--model_name_or_path\", default='roberta-large')\n",
        "parser.add_argument(\"--result_file\", type=str, default=\"sfs_scripts/results_fewshot_manual_kpt.txt\")\n",
        "parser.add_argument(\"--openprompt_path\", type=str, default=\"datasets\")\n",
        "\n",
        "parser.add_argument(\"--verbalizer\", default='manual',type=str)\n",
        "parser.add_argument(\"--calibration\", action=\"store_true\")\n",
        "parser.add_argument(\"--nocut\", action=\"store_true\")\n",
        "parser.add_argument(\"--filter\", default=\"none\", type=str)\n",
        "parser.add_argument(\"--template_id\", default=1,type=int)\n",
        "parser.add_argument(\"--max_token_split\", default=-1, type=int)\n",
        "parser.add_argument(\"--dataset\",default='agnews',type=str)\n",
        "parser.add_argument(\"--write_filter_record\", action=\"store_true\")\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "from openprompt.utils.reproduciblity import set_seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "if args.dataset == \"agnews\":\n",
        "    dataset['train'] = AgnewsProcessor().get_train_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    dataset['test'] = AgnewsProcessor().get_test_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    class_labels =AgnewsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/agnews\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 10\n",
        "elif args.dataset == \"dbpedia\":\n",
        "    dataset['train'] = DBpediaProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    dataset['test'] = DBpediaProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    class_labels =DBpediaProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/dbpedia\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"yahoo\":\n",
        "    dataset['train'] = YahooAnswersTopicsProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    dataset['test'] = YahooAnswersTopicsProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    class_labels =YahooAnswersTopicsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/yahoo_answers_topics\"\n",
        "    scriptformat = \"json\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"imdb\":\n",
        "    dataset['train'] = ImdbProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    dataset['test'] = ImdbProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    class_labels = ImdbProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/imdb\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "elif args.dataset == \"amazon\":\n",
        "    dataset['train'] = AmazonProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    dataset['test'] = AmazonProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    class_labels = AmazonProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/amazon\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template.txt\", choice=args.template_id)\n",
        "# mytemplate=ManualTemplate(\n",
        "#     text = '{\"placeholder\":\"text_a\"} It was {\"mask\"}',\n",
        "#     tokenizer = tokenizer,\n",
        "# )\n",
        "\n",
        "if args.verbalizer == \"kpt\":\n",
        "    myverbalizer = KnowledgeableVerbalizer(tokenizer, classes=class_labels, candidate_frac=cutoff, max_token_split=args.max_token_split).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/knowledgeable_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"manual\":\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"soft\":\n",
        "    raise NotImplementedError\n",
        "elif args.verbalizer == \"auto\":\n",
        "    raise NotImplementedError\n",
        "\n",
        "# (contextual) calibration\n",
        "if args.calibration:\n",
        "    from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "    support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)\n",
        "    dataset['support'] = support_sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "    for example in dataset['support']:\n",
        "        example.label = -1 # remove the labels of support set for clarification\n",
        "    support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=mytemplate, tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "        batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "        truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False, plm_eval_mode=args.plm_eval_mode)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "\n",
        "myrecord = \"\"\n",
        "# HP\n",
        "if args.calibration:\n",
        "    org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    from contextualize_calibration import calibrate\n",
        "    # calculate the calibration logits\n",
        "    cc_logits = calibrate(prompt_model, support_dataloader)\n",
        "    print(\"the calibration logits is\", cc_logits)\n",
        "    myrecord += \"Phase 1 {}\\n\".format(org_label_words_num)\n",
        "\n",
        "    myverbalizer.register_calibrate_logits(cc_logits.mean(dim=0))\n",
        "    new_label_words_num = [len(myverbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    myrecord += \"Phase 2 {}\\n\".format(new_label_words_num)\n",
        "\n",
        "    #relevance refinement adapts tf_idf\n",
        "    from filter_method import *\n",
        "    if args.filter == \"tfidf_filter\":\n",
        "        record = tfidf_filter(myverbalizer, cc_logits, class_labels)\n",
        "        myrecord += record\n",
        "    elif args.filter == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
        "    # currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
        "\n",
        "#\n",
        "if args.write_filter_record:\n",
        "    record_prefix = \"=\"*20+\"\\n\"\n",
        "    record_prefix += f\"dataset {args.dataset}\\t\"\n",
        "    record_prefix += f\"temp {args.template_id}\\t\"\n",
        "    record_prefix += f\"seed {args.seed}\\t\"\n",
        "    record_prefix += f\"cali {args.calibration}\\t\"\n",
        "    record_prefix += f\"filt {args.filter}\\t\"\n",
        "    record_prefix += \"\\n\"\n",
        "    myrecord = record_prefix +myrecord\n",
        "    with open(\"../sfs_scripts/filter_record_file.txt\",'a')  as fout_rec:\n",
        "        fout_rec.write(myrecord)\n",
        "    exit()\n",
        "\n",
        "\n",
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "pbar = tqdm(test_dataloader)\n",
        "for step, inputs in enumerate(pbar):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "recall=sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(alllabels)\n",
        "micro_f1=2*acc*recall/(acc+recall)\n",
        "'''\n",
        "pre=TP / (TP + FP)\n",
        "R=TP/ (TP+FN)\n",
        "F1=2PR/(P+R)\n",
        "\n",
        "'''\n",
        "  # roughly ~0.853 when using template 0\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"nocut {args.nocut}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n",
        "print('recall',recall)\n",
        "print('micro-f1',micro_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "recall=sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(alllabels)\n",
        "micro_f1=2*acc*recall/(acc+recall)\n",
        "recall,micro_f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UfRSCdIsQ1H",
        "outputId": "dc5f2862-e452-46d9-972e-cf2bc4fd08fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7890789473684211, 0.7890789473684212)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **template_id=0**"
      ],
      "metadata": {
        "id": "vqz6DnndtHiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 论文环境在的实验结果（zero-shot）"
      ],
      "metadata": {
        "id": "707fQV-7bsyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "按git-hub中下载的脚本的配置"
      ],
      "metadata": {
        "id": "lMNEQhTia5T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wQAPLO6utK1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581,
          "referenced_widgets": [
            "c1faf5a1661a4ffb824cd46bd637c3a2",
            "891ad6e4ef114752801917f9927d7627",
            "f3d1b69fe3d54f47b2fd02b0429aace1",
            "6a6bb9abddaf48d2bcdc97414d5a0f81",
            "10d07cce166c4debb742358c85b3d266",
            "2dcc49ffe1724a61806cf5d3dffd9c4f",
            "5a2acdaf71ff4083801a6c56dfffb847",
            "4a3edffe39aa46d785dcd20499bc4e64",
            "7a16716ffad64244bc5385638b28a3b3",
            "7e4085bcbaa74752b8149448eaab0240",
            "930c8cb1b5e74522adc80286e45cb8c2",
            "95a923da522c4ae3a78afbe1bb574eed",
            "f506fa1e2f62426092844f2c05b6dab7",
            "93f12efe182b43a781c4c4cabad88538",
            "7b72120034f245148b808ee0a187a286",
            "41e362461fec4511959185e0df9d8d45",
            "79cb246ca8464af9a602dad7679bbeb7",
            "e5f36445c2ed43f3bac255e2f84791e7",
            "978d83bebd7448bc93650019631f6fa4",
            "2fb643cec663471b9050307932f81f01",
            "e021cb74cdaa48c1a42464adc6e36227",
            "734536273a484d55ab61303cda016a66",
            "7318d1e8b83c44008bbc1cf6400738de",
            "cba3c880ce8d45b79fae045437add13b",
            "048d236d7f8a404fa22c5469ddd45143",
            "697e84f737524ee483337fe4c545f300",
            "0fa0b1e2ee7a48bcb4a37ff09044cfb1",
            "5584c8a7422c410da1acdb27f64144ac",
            "02f964fba99e4f14981d8b4cdc62a839",
            "047ece516027460ea6c3d05c7ab81d8a",
            "804e91282fcc44a2b79958ca675e62d6",
            "1e8ecc25da7d49db8a9305c703387ff3",
            "1f16ddde56f04ea685198ec4cab3d97f",
            "6234930a416547d5b9231c7d540384d1",
            "c3de32d627ce46729315fd8d6ff36257",
            "14311915451e4c23872c622349e8f966",
            "6bbfa3145c52415e9caa90753bdc2cec",
            "4b5d283aa310419b91beb9ab7b51f8bc",
            "3932bb7ee0d54fb192ebc046b5eb40b1",
            "e7e49af270d24eec809aa94ee00e45d6",
            "6bcdea6f5e524dd79aa604bc8810d5f9",
            "2a50cc4546244771ab2d9ec2d14db3d4",
            "b1c14f2bfb594b6c942e2549ac45f766",
            "199d9bb9166a4b2b84320a629a61ec95"
          ]
        },
        "outputId": "1d571ac6-30f3-4a38-8fb5-adfce6065150",
        "id": "1dTftcIRtLA2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1faf5a1661a4ffb824cd46bd637c3a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95a923da522c4ae3a78afbe1bb574eed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7318d1e8b83c44008bbc1cf6400738de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6234930a416547d5b9231c7d540384d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##Num of label words for each label: [376, 350, 287, 366]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 200it [00:00, 452.24it/s]\n",
            "ContextCali: 100%|██████████| 20/20 [00:03<00:00,  6.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the calibration logits is tensor([[33.6367, -4.4846, 45.7015,  ..., -0.3739, -0.1965, 25.9612],\n",
            "        [31.4200, -3.7351, 44.5797,  ...,  0.7121,  1.0803, 25.0002],\n",
            "        [34.7010, -4.0117, 46.3250,  ...,  0.3138,  1.7178, 26.9313],\n",
            "        ...,\n",
            "        [32.8307, -4.9485, 44.5945,  ..., -0.7820, -0.3619, 25.7531],\n",
            "        [35.0306, -4.8419, 45.5432,  ..., -0.3878,  0.4692, 27.0337],\n",
            "        [32.5885, -4.1696, 46.6274,  ...,  0.5883,  1.3667, 25.0157]],\n",
            "       device='cuda:0')\n",
            "##Num of label words for each label: [228, 269, 231, 245]\n",
            "norm_ord 5.975124378109453\n",
            "optimal_cut rate is 0.6579925650557621\n",
            "##Num of label words for each label: [150, 177, 151, 161]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 7600it [00:12, 612.62it/s]\n",
            "100%|██████████| 760/760 [01:53<00:00,  6.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "dataset agnews\ttemp 0\tseed 144\tverb kpt\tcali --calibration\tfilt tfidf_filter\tnocut False\tmaxsplit -1\t\n",
            "Acc: 0.8669736842105263\n",
            "\n",
            "\n",
            "recall 0.8669736842105263\n",
            "micro-f1 0.8669736842105263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor, DBpediaProcessor, ImdbProcessor, AmazonProcessor\n",
        "from openprompt.data_utils.huggingface_dataset import YahooAnswersTopicsProcessor\n",
        "import torch\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer\n",
        "from openprompt.prompts import ManualTemplate\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"\")\n",
        "parser.add_argument(\"--shot\", type=int, default=5)\n",
        "parser.add_argument(\"--seed\", type=int, default=144)\n",
        "\n",
        "parser.add_argument(\"--plm_eval_mode\", action=\"store_true\")\n",
        "parser.add_argument(\"--model\", type=str, default='roberta')\n",
        "parser.add_argument(\"--model_name_or_path\", default='roberta-large')\n",
        "parser.add_argument(\"--result_file\", type=str, default=\"sfs_scripts/results_fewshot_manual_kpt.txt\")\n",
        "parser.add_argument(\"--openprompt_path\", type=str, default=\"datasets\")\n",
        "\n",
        "parser.add_argument(\"--verbalizer\", default='kpt',type=str)\n",
        "parser.add_argument(\"--calibration\", default='--calibration',action=\"store_true\")\n",
        "parser.add_argument(\"--nocut\", action=\"store_true\")\n",
        "parser.add_argument(\"--filter\", default=\"tfidf_filter\", type=str)\n",
        "parser.add_argument(\"--template_id\", default=0,type=int)\n",
        "parser.add_argument(\"--max_token_split\", default=-1, type=int)\n",
        "parser.add_argument(\"--dataset\",default='agnews',type=str)\n",
        "parser.add_argument(\"--write_filter_record\", action=\"store_true\")\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "from openprompt.utils.reproduciblity import set_seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "if args.dataset == \"agnews\":\n",
        "    dataset['train'] = AgnewsProcessor().get_train_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    dataset['test'] = AgnewsProcessor().get_test_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    class_labels =AgnewsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/agnews\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 10\n",
        "elif args.dataset == \"dbpedia\":\n",
        "    dataset['train'] = DBpediaProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    dataset['test'] = DBpediaProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    class_labels =DBpediaProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/dbpedia\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"yahoo\":\n",
        "    dataset['train'] = YahooAnswersTopicsProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    dataset['test'] = YahooAnswersTopicsProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    class_labels =YahooAnswersTopicsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/yahoo_answers_topics\"\n",
        "    scriptformat = \"json\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"imdb\":\n",
        "    dataset['train'] = ImdbProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    dataset['test'] = ImdbProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    class_labels = ImdbProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/imdb\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "elif args.dataset == \"amazon\":\n",
        "    dataset['train'] = AmazonProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    dataset['test'] = AmazonProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    class_labels = AmazonProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/amazon\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template.txt\", choice=args.template_id)\n",
        "# mytemplate=ManualTemplate(\n",
        "#     text = '{\"placeholder\":\"text_a\"} It was {\"mask\"}',\n",
        "#     tokenizer = tokenizer,\n",
        "# )\n",
        "\n",
        "if args.verbalizer == \"kpt\":\n",
        "    myverbalizer = KnowledgeableVerbalizer(tokenizer, classes=class_labels, candidate_frac=cutoff, max_token_split=args.max_token_split).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/knowledgeable_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"manual\":\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"soft\":\n",
        "    raise NotImplementedError\n",
        "elif args.verbalizer == \"auto\":\n",
        "    raise NotImplementedError\n",
        "\n",
        "# (contextual) calibration\n",
        "if args.calibration:\n",
        "    from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "    support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)\n",
        "    dataset['support'] = support_sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "    for example in dataset['support']:\n",
        "        example.label = -1 # remove the labels of support set for clarification\n",
        "    support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=mytemplate, tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "        batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "        truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False, plm_eval_mode=args.plm_eval_mode)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "\n",
        "myrecord = \"\"\n",
        "# HP\n",
        "if args.calibration:\n",
        "    org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    from contextualize_calibration import calibrate\n",
        "    # calculate the calibration logits\n",
        "    cc_logits = calibrate(prompt_model, support_dataloader)\n",
        "    print(\"the calibration logits is\", cc_logits)\n",
        "    myrecord += \"Phase 1 {}\\n\".format(org_label_words_num)\n",
        "\n",
        "    myverbalizer.register_calibrate_logits(cc_logits.mean(dim=0))\n",
        "    new_label_words_num = [len(myverbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    myrecord += \"Phase 2 {}\\n\".format(new_label_words_num)\n",
        "\n",
        "    #relevance refinement adapts tf_idf\n",
        "    from filter_method import *\n",
        "    if args.filter == \"tfidf_filter\":\n",
        "        record = tfidf_filter(myverbalizer, cc_logits, class_labels)\n",
        "        myrecord += record\n",
        "    elif args.filter == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
        "    # currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
        "\n",
        "#\n",
        "if args.write_filter_record:\n",
        "    record_prefix = \"=\"*20+\"\\n\"\n",
        "    record_prefix += f\"dataset {args.dataset}\\t\"\n",
        "    record_prefix += f\"temp {args.template_id}\\t\"\n",
        "    record_prefix += f\"seed {args.seed}\\t\"\n",
        "    record_prefix += f\"cali {args.calibration}\\t\"\n",
        "    record_prefix += f\"filt {args.filter}\\t\"\n",
        "    record_prefix += \"\\n\"\n",
        "    myrecord = record_prefix +myrecord\n",
        "    with open(\"../sfs_scripts/filter_record_file.txt\",'a')  as fout_rec:\n",
        "        fout_rec.write(myrecord)\n",
        "    exit()\n",
        "\n",
        "\n",
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "pbar = tqdm(test_dataloader)\n",
        "for step, inputs in enumerate(pbar):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "recall=sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(alllabels)\n",
        "micro_f1=2*acc*recall/(acc+recall)\n",
        "'''\n",
        "pre=TP / (TP + FP)\n",
        "R=TP/ (TP+FN)\n",
        "F1=2PR/(P+R)\n",
        "\n",
        "'''\n",
        "  # roughly ~0.853 when using template 0\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"nocut {args.nocut}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n",
        "print('recall',recall)\n",
        "print('micro-f1',micro_f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using filter refine & Carli refine "
      ],
      "metadata": {
        "id": "uECQc2rstWSX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "03fbbdd0-cd47-4320-8064-03dbad2bd6d0",
        "id": "DAPLqERqtgzr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 200it [00:00, 627.25it/s]\n",
            "ContextCali: 100%|██████████| 20/20 [00:02<00:00,  6.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the calibration logits is tensor([[33.6367, -4.4846, 45.7015,  ..., -0.3739, -0.1965, 25.9612],\n",
            "        [31.4200, -3.7351, 44.5797,  ...,  0.7121,  1.0803, 25.0002],\n",
            "        [34.7010, -4.0117, 46.3250,  ...,  0.3138,  1.7178, 26.9313],\n",
            "        ...,\n",
            "        [32.8307, -4.9485, 44.5945,  ..., -0.7820, -0.3619, 25.7531],\n",
            "        [35.0306, -4.8419, 45.5432,  ..., -0.3878,  0.4692, 27.0337],\n",
            "        [32.5885, -4.1696, 46.6274,  ...,  0.5883,  1.3667, 25.0157]],\n",
            "       device='cuda:0')\n",
            "norm_ord 5.975124378109453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-da53b0bb931d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mfilter_method\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tfidf_filter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyverbalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcc_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0mmyrecord\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"none\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/filter_method.py\u001b[0m in \u001b[0;36mtfidf_filter\u001b[0;34m(myverbalizer, cc_logits, class_labels)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlabel_words_cc_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_words_cc_logits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabel_words_cc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_words_cc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfirst_label_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_words_cc_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0morgshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_words_cc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlabel_words_cc_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_words_cc_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor, DBpediaProcessor, ImdbProcessor, AmazonProcessor\n",
        "from openprompt.data_utils.huggingface_dataset import YahooAnswersTopicsProcessor\n",
        "import torch\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer\n",
        "from openprompt.prompts import ManualTemplate\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"\")\n",
        "parser.add_argument(\"--shot\", type=int, default=0)\n",
        "parser.add_argument(\"--seed\", type=int, default=144)\n",
        "\n",
        "parser.add_argument(\"--plm_eval_mode\", action=\"store_true\")\n",
        "parser.add_argument(\"--model\", type=str, default='roberta')\n",
        "parser.add_argument(\"--model_name_or_path\", default='roberta-large')\n",
        "parser.add_argument(\"--result_file\", type=str, default=\"sfs_scripts/results_fewshot_manual_kpt.txt\")\n",
        "parser.add_argument(\"--openprompt_path\", type=str, default=\"datasets\")\n",
        "\n",
        "parser.add_argument(\"--verbalizer\", default='manual',type=str)\n",
        "parser.add_argument(\"--calibration\", default='True',action=\"store_true\")\n",
        "parser.add_argument(\"--nocut\", action=\"store_true\")\n",
        "parser.add_argument(\"--filter\", default=\"tfidf_filter\", type=str)\n",
        "parser.add_argument(\"--template_id\", default=0,type=int)\n",
        "parser.add_argument(\"--max_token_split\", default=-1, type=int)\n",
        "parser.add_argument(\"--dataset\",default='agnews',type=str)\n",
        "parser.add_argument(\"--write_filter_record\", action=\"store_true\")\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "from openprompt.utils.reproduciblity import set_seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "if args.dataset == \"agnews\":\n",
        "    dataset['train'] = AgnewsProcessor().get_train_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    dataset['test'] = AgnewsProcessor().get_test_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    class_labels =AgnewsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/agnews\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 10\n",
        "elif args.dataset == \"dbpedia\":\n",
        "    dataset['train'] = DBpediaProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    dataset['test'] = DBpediaProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    class_labels =DBpediaProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/dbpedia\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"yahoo\":\n",
        "    dataset['train'] = YahooAnswersTopicsProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    dataset['test'] = YahooAnswersTopicsProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    class_labels =YahooAnswersTopicsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/yahoo_answers_topics\"\n",
        "    scriptformat = \"json\"\n",
        "    cutoff=0.5 if (not args.nocut) else 0.0\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"imdb\":\n",
        "    dataset['train'] = ImdbProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    dataset['test'] = ImdbProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    class_labels = ImdbProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/imdb\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "elif args.dataset == \"amazon\":\n",
        "    dataset['train'] = AmazonProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    dataset['test'] = AmazonProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    class_labels = AmazonProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/amazon\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template.txt\", choice=args.template_id)\n",
        "# mytemplate=ManualTemplate(\n",
        "#     text = '{\"placeholder\":\"text_a\"} It was {\"mask\"}',\n",
        "#     tokenizer = tokenizer,\n",
        "# )\n",
        "\n",
        "if args.verbalizer == \"kpt\":\n",
        "    myverbalizer = KnowledgeableVerbalizer(tokenizer, classes=class_labels, candidate_frac=cutoff, max_token_split=args.max_token_split).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/knowledgeable_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"manual\":\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"soft\":\n",
        "    raise NotImplementedError\n",
        "elif args.verbalizer == \"auto\":\n",
        "    raise NotImplementedError\n",
        "\n",
        "# (contextual) calibration\n",
        "if args.calibration:\n",
        "    from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "    support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)\n",
        "    dataset['support'] = support_sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "    for example in dataset['support']:\n",
        "        example.label = -1 # remove the labels of support set for clarification\n",
        "    support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=mytemplate, tokenizer=tokenizer,\n",
        "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "        batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "        truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False, plm_eval_mode=args.plm_eval_mode)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "\n",
        "myrecord = \"\"\n",
        "# HP\n",
        "if args.calibration:\n",
        "    org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    from contextualize_calibration import calibrate\n",
        "    # calculate the calibration logits\n",
        "    cc_logits = calibrate(prompt_model, support_dataloader)\n",
        "    print(\"the calibration logits is\", cc_logits)\n",
        "    myrecord += \"Phase 1 {}\\n\".format(org_label_words_num)\n",
        "\n",
        "    myverbalizer.register_calibrate_logits(cc_logits.mean(dim=0))\n",
        "    new_label_words_num = [len(myverbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "    myrecord += \"Phase 2 {}\\n\".format(new_label_words_num)\n",
        "\n",
        "    #relevance refinement adapts tf_idf\n",
        "    from filter_method import *\n",
        "    if args.filter == \"tfidf_filter\":\n",
        "        record = tfidf_filter(myverbalizer, cc_logits, class_labels)\n",
        "        myrecord += record\n",
        "    elif args.filter == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
        "    # currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
        "\n",
        "#\n",
        "if args.write_filter_record:\n",
        "    record_prefix = \"=\"*20+\"\\n\"\n",
        "    record_prefix += f\"dataset {args.dataset}\\t\"\n",
        "    record_prefix += f\"temp {args.template_id}\\t\"\n",
        "    record_prefix += f\"seed {args.seed}\\t\"\n",
        "    record_prefix += f\"cali {args.calibration}\\t\"\n",
        "    record_prefix += f\"filt {args.filter}\\t\"\n",
        "    record_prefix += \"\\n\"\n",
        "    myrecord = record_prefix +myrecord\n",
        "    with open(\"../sfs_scripts/filter_record_file.txt\",'a')  as fout_rec:\n",
        "        fout_rec.write(myrecord)\n",
        "    exit()\n",
        "\n",
        "\n",
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "allpreds = []\n",
        "alllabels = []\n",
        "pbar = tqdm(test_dataloader)\n",
        "for step, inputs in enumerate(pbar):\n",
        "    if use_cuda:\n",
        "        inputs = inputs.cuda()\n",
        "    logits = prompt_model(inputs)\n",
        "    labels = inputs['label']\n",
        "    alllabels.extend(labels.cpu().tolist())\n",
        "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "recall=sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(alllabels)\n",
        "micro_f1=2*acc*recall/(acc+recall)\n",
        "'''\n",
        "pre=TP / (TP + FP)\n",
        "R=TP/ (TP+FN)\n",
        "F1=2PR/(P+R)\n",
        "\n",
        "'''\n",
        "  # roughly ~0.853 when using template 0\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"nocut {args.nocut}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n",
        "print('recall',recall)\n",
        "print('micro-f1',micro_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"{args.result_file}\", \"a\") as fout:\n",
        "    fout.write(content_write)"
      ],
      "metadata": {
        "id": "DHF0518A9_6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(allpreds[:10],alllabels[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7rpbWGq-MRx",
        "outputId": "bd81ae63-a8d0-43f5-94fb-348f348f4773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 3, 0, 2, 0, 3, 3, 3, 3] [2, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# few_shot"
      ],
      "metadata": {
        "id": "2q_JNPuAvkjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor, DBpediaProcessor, ImdbProcessor, AmazonProcessor\n",
        "from openprompt.data_utils.huggingface_dataset import YahooAnswersTopicsProcessor\n",
        "import torch\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer, SoftVerbalizer, AutomaticVerbalizer\n",
        "from openprompt.prompts import ManualTemplate\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"\")\n",
        "\n",
        "parser.add_argument(\"--model\", type=str, default='roberta')\n",
        "parser.add_argument(\"--model_name_or_path\", default='roberta-large')\n",
        "parser.add_argument(\"--result_file\", type=str, default=\"sfs_scripts/results_fewshot_manual_kpt.txt\")\n",
        "parser.add_argument(\"--openprompt_path\", type=str, default=\"datasets\")\n",
        "\n",
        "parser.add_argument(\"--shot\", type=int, default=5)\n",
        "parser.add_argument(\"--seed\", type=int, default=144)\n",
        "parser.add_argument(\"--plm_eval_mode\",default=True, action=\"store_true\")\n",
        "parser.add_argument(\"--verbalizer\",default='manual', type=str)\n",
        "parser.add_argument(\"--calibration\",default=True, action=\"store_true\")\n",
        "parser.add_argument(\"--filter\", default=\"none\", type=str)\n",
        "parser.add_argument(\"--template_id\", default=1,type=int)\n",
        "parser.add_argument(\"--dataset\",default='agnews',type=str)\n",
        "\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--kptw_lr\", default=0.06, type=float)\n",
        "parser.add_argument(\"--pred_temp\", default=1.0, type=float)\n",
        "parser.add_argument(\"--max_token_split\", default=-1, type=int)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "import random\n",
        "this_run_unicode = str(random.randint(0, 1e10))\n",
        "\n",
        "from openprompt.utils.reproduciblity import set_seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "if args.dataset == \"agnews\":\n",
        "    dataset['train'] = AgnewsProcessor().get_train_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    dataset['test'] = AgnewsProcessor().get_test_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    class_labels =AgnewsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/agnews\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 10\n",
        "elif args.dataset == \"dbpedia\":\n",
        "    dataset['train'] = DBpediaProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    dataset['test'] = DBpediaProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    class_labels =DBpediaProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/dbpedia\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"yahoo\":\n",
        "    dataset['train'] = YahooAnswersTopicsProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    dataset['test'] = YahooAnswersTopicsProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    class_labels =YahooAnswersTopicsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/yahoo_answers_topics\"\n",
        "    scriptformat = \"json\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"imdb\":\n",
        "    dataset['train'] = ImdbProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    dataset['test'] = ImdbProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    class_labels = ImdbProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/imdb\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "elif args.dataset == \"amazon\":\n",
        "    dataset['train'] = AmazonProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    dataset['test'] = AmazonProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    class_labels = AmazonProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/amazon\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template.txt\", choice=args.template_id)\n",
        "\n",
        "\n",
        "if args.verbalizer == \"kpt\":\n",
        "    myverbalizer = KnowledgeableVerbalizer(tokenizer, classes=class_labels, candidate_frac=cutoff, pred_temp=args.pred_temp, max_token_split=args.max_token_split).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/knowledgeable_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"manual\":\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"soft\":\n",
        "    myverbalizer = SoftVerbalizer(tokenizer, model=plm, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"auto\":\n",
        "    myverbalizer = AutomaticVerbalizer(tokenizer, classes=class_labels)\n",
        "\n",
        "\n",
        "# (contextual) calibration\n",
        "if args.verbalizer in [\"kpt\",\"manual\"]:\n",
        "    if args.calibration or args.filter != \"none\":\n",
        "        from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "        support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)\n",
        "        dataset['support'] = support_sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "        # for example in dataset['support']:\n",
        "        #     example.label = -1 # remove the labels of support set for clarification\n",
        "        support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=mytemplate, tokenizer=tokenizer,\n",
        "            tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "            batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "            truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False, plm_eval_mode=args.plm_eval_mode)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# HP\n",
        "# if args.calibration:\n",
        "if args.verbalizer in [\"kpt\",\"manual\"]:\n",
        "    if args.calibration or args.filter != \"none\":\n",
        "        org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "        from contextualize_calibration import calibrate\n",
        "        # calculate the calibration logits\n",
        "        cc_logits = calibrate(prompt_model, support_dataloader)\n",
        "        print(\"the calibration logits is\", cc_logits)\n",
        "        print(\"origial label words num {}\".format(org_label_words_num))\n",
        "\n",
        "    if args.calibration:\n",
        "        myverbalizer.register_calibrate_logits(cc_logits.mean(dim=0))\n",
        "        new_label_words_num = [len(myverbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "        print(\"After filtering, number of label words per class: {}\".format(new_label_words_num))\n",
        "\n",
        "\n",
        "    from filter_method import *\n",
        "    if args.filter == \"tfidf_filter\":\n",
        "        tfidf_filter(myverbalizer, cc_logits, class_labels)\n",
        "    elif args.filter == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
        "    # currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
        "\n",
        "from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "sampler = FewShotSampler(num_examples_per_label=args.shot, also_sample_dev=True, num_examples_per_label_dev=args.shot)\n",
        "dataset['train'], dataset['validation'] = sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "def evaluate(prompt_model, dataloader, desc):\n",
        "    prompt_model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    pbar = tqdm(dataloader, desc=desc)\n",
        "    for step, inputs in enumerate(pbar):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        alllabels.extend(labels.cpu().tolist())\n",
        "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "\n",
        "    recall=sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(alllabels)\n",
        "    micro_f1=2*acc*recall/(acc+recall)\n",
        "\n",
        "    return acc,recall,micro_f1\n",
        "############\n",
        "#############\n",
        "###############\n",
        "\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def prompt_initialize(verbalizer, prompt_model, init_dataloader):\n",
        "    dataloader = init_dataloader\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Init_using_{}\".format(\"train\")):\n",
        "            batch = batch.cuda()\n",
        "            logits = prompt_model(batch)\n",
        "        verbalizer.optimize_to_initialize()\n",
        "'''\n",
        "This is an epoch-level optimize. If used in batch-level like an ordinary gradient descend optimizer,\n",
        " the result may not be very satisfying since the accumated examples (i.e., the probs_buffer and the labels_buffer) are not enough if the batchsize is small.'''\n",
        "\n",
        "if args.verbalizer == \"soft\":\n",
        "\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer_grouped_parameters2 = [\n",
        "        {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
        "        {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4},\n",
        "    ]\n",
        "\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "    optimizer2 = AdamW(optimizer_grouped_parameters2)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    scheduler2 = get_linear_schedule_with_warmup(\n",
        "        optimizer2,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "elif args.verbalizer == \"auto\":\n",
        "    prompt_initialize(myverbalizer, prompt_model, train_dataloader)\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    optimizer2 = None\n",
        "    scheduler2 = None\n",
        "\n",
        "elif args.verbalizer == \"kpt\":\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    # optimizer_grouped_parameters2 = [\n",
        "    #     {'params': , \"lr\":1e-1},\n",
        "    # ]\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "    optimizer2 = AdamW(prompt_model.verbalizer.parameters(), lr=args.kptw_lr)\n",
        "    # print(optimizer_grouped_parameters2)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    # scheduler2 = get_linear_schedule_with_warmup(\n",
        "    #     optimizer2,\n",
        "    #     num_warmup_steps=0, num_training_steps=tot_step)\n",
        "    scheduler2 = None\n",
        "\n",
        "elif args.verbalizer == \"manual\":\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    optimizer2 = None\n",
        "    scheduler2 = None\n",
        "\n",
        "\n",
        "tot_loss = 0\n",
        "log_loss = 0\n",
        "best_val_acc = 0\n",
        "for epoch in range(args.max_epochs):\n",
        "    tot_loss = 0\n",
        "    prompt_model.train()\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)#bs*class_nums\n",
        "        labels = inputs['label']#bs*1\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.requires_grad_(True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
        "        tot_loss += loss.item()\n",
        "        optimizer1.step()\n",
        "        scheduler1.step()\n",
        "        optimizer1.zero_grad()\n",
        "        if optimizer2 is not None:\n",
        "            optimizer2.step()\n",
        "            optimizer2.zero_grad()\n",
        "        if scheduler2 is not None:\n",
        "            scheduler2.step()\n",
        "    \n",
        "    val_acc,r,f = evaluate(prompt_model, validation_dataloader, desc=\"Valid\")\n",
        "    if val_acc>=best_val_acc:\n",
        "        torch.save(prompt_model.state_dict(),f\"ckpts/{this_run_unicode}.ckpt\")\n",
        "        best_val_acc = val_acc\n",
        "    print(\"Epoch {},train_loss  {} val_acc {}\".format(epoch,tot_loss/len(train_dataloader),val_acc), flush=True)\n",
        "\n",
        "prompt_model.load_state_dict(torch.load(f\"ckpts/{this_run_unicode}.ckpt\"))\n",
        "prompt_model = prompt_model.cuda()\n",
        "test_acc,test_r,test_f = evaluate(prompt_model, test_dataloader, desc=\"Test\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"shot {args.shot}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += f\"kptw_lr {args.kptw_lr}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {test_acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n",
        "print('Recall',test_r)\n",
        "print('F1 score',test_f)\n",
        "\n",
        "with open(f\"{args.result_file}\", \"a\") as fout:\n",
        "    fout.write(content_write)\n",
        "\n",
        "import os\n",
        "os.remove(f\"ckpts/{this_run_unicode}.ckpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "iQdgK42WwEOm",
        "outputId": "b2d674cb-c4e6-4246-d5d8-216b73bb0c77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 200it [00:00, 452.76it/s]\n",
            "ContextCali: 100%|██████████| 20/20 [00:02<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the calibration logits is tensor([[45.8878, -4.8530, 52.5500,  ..., -0.8933,  0.7490, 29.2852],\n",
            "        [32.2870, -3.5604, 41.0649,  ...,  0.0849,  1.1912, 22.0599],\n",
            "        [38.1036, -4.2718, 47.9503,  ...,  0.4094,  1.7225, 25.8820],\n",
            "        ...,\n",
            "        [28.5844, -4.0619, 38.6169,  ..., -0.8482,  0.5126, 20.3076],\n",
            "        [41.2795, -4.8353, 50.6907,  ..., -0.0834,  0.5635, 28.0692],\n",
            "        [39.1174, -5.0500, 49.1213,  ...,  0.6306,  0.9125, 27.0292]],\n",
            "       device='cuda:0')\n",
            "origial label words num [1, 1, 1, 1]\n",
            "After filtering, number of label words per class: [1, 1, 1, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 20it [00:00, 437.05it/s]\n",
            "tokenizing: 20it [00:00, 478.92it/s]\n",
            "tokenizing: 7600it [00:12, 606.66it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,train_loss  0.47282166220247746 val_acc 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1,train_loss  0.47282160073518753 val_acc 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2,train_loss  0.4728216528892517 val_acc 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3,train_loss  0.4728216379880905 val_acc 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4,train_loss  0.4728216379880905 val_acc 0.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 760/760 [01:52<00:00,  6.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "dataset agnews\ttemp 1\tseed 144\tshot 5\tverb manual\tcali True\tfilt none\tmaxsplit -1\tkptw_lr 0.06\t\n",
            "Acc: 0.7967105263157894\n",
            "\n",
            "\n",
            "Recall 0.7967105263157894\n",
            "F1 score 0.7967105263157893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a4b451bd928b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1 score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{args.result_file}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sfs_scripts/results_fewshot_manual_kpt.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few_shot(论文环境）"
      ],
      "metadata": {
        "id": "JEI32vz4fPff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor, DBpediaProcessor, ImdbProcessor, AmazonProcessor\n",
        "from openprompt.data_utils.huggingface_dataset import YahooAnswersTopicsProcessor\n",
        "import torch\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer, SoftVerbalizer, AutomaticVerbalizer\n",
        "from openprompt.prompts import ManualTemplate\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"\")\n",
        "\n",
        "parser.add_argument(\"--model\", type=str, default='roberta')\n",
        "parser.add_argument(\"--model_name_or_path\", default='roberta-large')\n",
        "parser.add_argument(\"--result_file\", type=str, default=\"sfs_scripts/results_fewshot_manual_kpt.txt\")\n",
        "parser.add_argument(\"--openprompt_path\", type=str, default=\"datasets\")\n",
        "\n",
        "parser.add_argument(\"--shot\", type=int, default=5)\n",
        "parser.add_argument(\"--seed\", type=int, default=144)\n",
        "parser.add_argument(\"--plm_eval_mode\",default=True, action=\"store_true\")\n",
        "parser.add_argument(\"--verbalizer\",default='kpt', type=str)\n",
        "parser.add_argument(\"--calibration\",default=True, action=\"store_true\")\n",
        "parser.add_argument(\"--filter\", default=\"tfidf_filter\", type=str)\n",
        "parser.add_argument(\"--template_id\", default=0,type=int)\n",
        "parser.add_argument(\"--dataset\",default='agnews',type=str)\n",
        "\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=5)\n",
        "parser.add_argument(\"--kptw_lr\", default=0, type=float)\n",
        "parser.add_argument(\"--pred_temp\", default=1.0, type=float)\n",
        "parser.add_argument(\"--max_token_split\", default=-1, type=int)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "import random\n",
        "this_run_unicode = str(random.randint(0, 1e10))\n",
        "\n",
        "from openprompt.utils.reproduciblity import set_seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "if args.dataset == \"agnews\":\n",
        "    dataset['train'] = AgnewsProcessor().get_train_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    dataset['test'] = AgnewsProcessor().get_test_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    class_labels =AgnewsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/agnews\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 10\n",
        "elif args.dataset == \"dbpedia\":\n",
        "    dataset['train'] = DBpediaProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    dataset['test'] = DBpediaProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    class_labels =DBpediaProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/dbpedia\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"yahoo\":\n",
        "    dataset['train'] = YahooAnswersTopicsProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    dataset['test'] = YahooAnswersTopicsProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    class_labels =YahooAnswersTopicsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/yahoo_answers_topics\"\n",
        "    scriptformat = \"json\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"imdb\":\n",
        "    dataset['train'] = ImdbProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    dataset['test'] = ImdbProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    class_labels = ImdbProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/imdb\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "elif args.dataset == \"amazon\":\n",
        "    dataset['train'] = AmazonProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    dataset['test'] = AmazonProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    class_labels = AmazonProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/amazon\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template.txt\", choice=args.template_id)\n",
        "\n",
        "\n",
        "if args.verbalizer == \"kpt\":\n",
        "    myverbalizer = KnowledgeableVerbalizer(tokenizer, classes=class_labels, candidate_frac=cutoff, pred_temp=args.pred_temp, max_token_split=args.max_token_split).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/knowledgeable_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"manual\":\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"soft\":\n",
        "    myverbalizer = SoftVerbalizer(tokenizer, model=plm, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"auto\":\n",
        "    myverbalizer = AutomaticVerbalizer(tokenizer, classes=class_labels)\n",
        "\n",
        "\n",
        "# (contextual) calibration\n",
        "if args.verbalizer in [\"kpt\",\"manual\"]:\n",
        "    if args.calibration or args.filter != \"none\":\n",
        "        from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "        support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)\n",
        "        dataset['support'] = support_sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "        # for example in dataset['support']:\n",
        "        #     example.label = -1 # remove the labels of support set for clarification\n",
        "        support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=mytemplate, tokenizer=tokenizer,\n",
        "            tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "            batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "            truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False, plm_eval_mode=args.plm_eval_mode)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# HP\n",
        "# if args.calibration:\n",
        "if args.verbalizer in [\"kpt\",\"manual\"]:\n",
        "    if args.calibration or args.filter != \"none\":\n",
        "        org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "        from contextualize_calibration import calibrate\n",
        "        # calculate the calibration logits\n",
        "        cc_logits = calibrate(prompt_model, support_dataloader)\n",
        "        print(\"the calibration logits is\", cc_logits)\n",
        "        print(\"origial label words num {}\".format(org_label_words_num))\n",
        "\n",
        "    if args.calibration:\n",
        "        myverbalizer.register_calibrate_logits(cc_logits.mean(dim=0))\n",
        "        new_label_words_num = [len(myverbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "        print(\"After filtering, number of label words per class: {}\".format(new_label_words_num))\n",
        "\n",
        "\n",
        "    from filter_method import *\n",
        "    if args.filter == \"tfidf_filter\":\n",
        "        tfidf_filter(myverbalizer, cc_logits, class_labels)\n",
        "    elif args.filter == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
        "    # currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
        "\n",
        "from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "sampler = FewShotSampler(num_examples_per_label=args.shot, also_sample_dev=True, num_examples_per_label_dev=args.shot)\n",
        "dataset['train'], dataset['validation'] = sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "def evaluate(prompt_model, dataloader, desc):\n",
        "    prompt_model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    pbar = tqdm(dataloader, desc=desc)\n",
        "    for step, inputs in enumerate(pbar):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        alllabels.extend(labels.cpu().tolist())\n",
        "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "\n",
        "    recall=sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(alllabels)\n",
        "    micro_f1=2*acc*recall/(acc+recall)\n",
        "\n",
        "    return acc,recall,micro_f1\n",
        "############\n",
        "#############\n",
        "###############\n",
        "\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def prompt_initialize(verbalizer, prompt_model, init_dataloader):\n",
        "    dataloader = init_dataloader\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Init_using_{}\".format(\"train\")):\n",
        "            batch = batch.cuda()\n",
        "            logits = prompt_model(batch)\n",
        "        verbalizer.optimize_to_initialize()\n",
        "'''\n",
        "This is an epoch-level optimize. If used in batch-level like an ordinary gradient descend optimizer,\n",
        " the result may not be very satisfying since the accumated examples (i.e., the probs_buffer and the labels_buffer) are not enough if the batchsize is small.'''\n",
        "\n",
        "if args.verbalizer == \"soft\":\n",
        "\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer_grouped_parameters2 = [\n",
        "        {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
        "        {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4},\n",
        "    ]\n",
        "\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "    optimizer2 = AdamW(optimizer_grouped_parameters2)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    scheduler2 = get_linear_schedule_with_warmup(\n",
        "        optimizer2,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "elif args.verbalizer == \"auto\":\n",
        "    prompt_initialize(myverbalizer, prompt_model, train_dataloader)\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    optimizer2 = None\n",
        "    scheduler2 = None\n",
        "\n",
        "elif args.verbalizer == \"kpt\":\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    # optimizer_grouped_parameters2 = [\n",
        "    #     {'params': , \"lr\":1e-1},\n",
        "    # ]\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "    optimizer2 = AdamW(prompt_model.verbalizer.parameters(), lr=args.kptw_lr)\n",
        "    # print(optimizer_grouped_parameters2)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    # scheduler2 = get_linear_schedule_with_warmup(\n",
        "    #     optimizer2,\n",
        "    #     num_warmup_steps=0, num_training_steps=tot_step)\n",
        "    scheduler2 = None\n",
        "\n",
        "elif args.verbalizer == \"manual\":\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    optimizer2 = None\n",
        "    scheduler2 = None\n",
        "\n",
        "\n",
        "tot_loss = 0\n",
        "log_loss = 0\n",
        "best_val_acc = 0\n",
        "for epoch in range(args.max_epochs):\n",
        "    tot_loss = 0\n",
        "    prompt_model.train()\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)#bs*class_nums\n",
        "        labels = inputs['label']#bs*1\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.requires_grad_(True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
        "        tot_loss += loss.item()\n",
        "        optimizer1.step()\n",
        "        scheduler1.step()\n",
        "        optimizer1.zero_grad()\n",
        "        if optimizer2 is not None:\n",
        "            optimizer2.step()\n",
        "            optimizer2.zero_grad()\n",
        "        if scheduler2 is not None:\n",
        "            scheduler2.step()\n",
        "    \n",
        "    val_acc,r,f = evaluate(prompt_model, validation_dataloader, desc=\"Valid\")\n",
        "    if val_acc>=best_val_acc:\n",
        "        torch.save(prompt_model.state_dict(),f\"ckpts/{this_run_unicode}.ckpt\")\n",
        "        best_val_acc = val_acc\n",
        "    print(\"Epoch {},train_loss  {} val_acc {}\".format(epoch,tot_loss/len(train_dataloader),val_acc), flush=True)\n",
        "\n",
        "prompt_model.load_state_dict(torch.load(f\"ckpts/{this_run_unicode}.ckpt\"))\n",
        "prompt_model = prompt_model.cuda()\n",
        "test_acc,test_r,test_f = evaluate(prompt_model, test_dataloader, desc=\"Test\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"shot {args.shot}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += f\"kptw_lr {args.kptw_lr}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {test_acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n",
        "print('Recall',test_r)\n",
        "print('F1 score',test_f)\n",
        "\n",
        "with open(f\"{args.result_file}\", \"a\") as fout:\n",
        "    fout.write(content_write)\n",
        "\n",
        "import os\n",
        "os.remove(f\"ckpts/{this_run_unicode}.ckpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a559d6-7cfa-4bdf-88d0-02231d7e3c37",
        "id": "z6zDnQ-9fToG"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##Num of label words for each label: [376, 350, 287, 366]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 200it [00:00, 471.91it/s]\n",
            "ContextCali: 100%|██████████| 20/20 [00:03<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the calibration logits is tensor([[33.6367, -4.4846, 45.7015,  ..., -0.3739, -0.1965, 25.9612],\n",
            "        [31.4200, -3.7351, 44.5797,  ...,  0.7121,  1.0803, 25.0002],\n",
            "        [34.7010, -4.0117, 46.3250,  ...,  0.3138,  1.7178, 26.9313],\n",
            "        ...,\n",
            "        [32.8307, -4.9485, 44.5945,  ..., -0.7820, -0.3619, 25.7531],\n",
            "        [35.0306, -4.8419, 45.5432,  ..., -0.3878,  0.4692, 27.0337],\n",
            "        [32.5885, -4.1696, 46.6274,  ...,  0.5883,  1.3667, 25.0157]],\n",
            "       device='cuda:0')\n",
            "origial label words num [376, 350, 287, 366]\n",
            "##Num of label words for each label: [228, 269, 231, 245]\n",
            "After filtering, number of label words per class: [228, 269, 231, 245]\n",
            "norm_ord 5.975124378109453\n",
            "optimal_cut rate is 0.6579925650557621\n",
            "##Num of label words for each label: [150, 177, 151, 161]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 20it [00:00, 473.59it/s]\n",
            "tokenizing: 20it [00:00, 474.26it/s]\n",
            "tokenizing: 7600it [00:12, 599.03it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,train_loss  0.5654636472463608 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1,train_loss  0.5654636770486832 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2,train_loss  0.5654636919498444 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3,train_loss  0.565463662147522 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4,train_loss  0.5654637217521667 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 760/760 [01:52<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "dataset agnews\ttemp 0\tseed 144\tshot 5\tverb kpt\tcali True\tfilt tfidf_filter\tmaxsplit -1\tkptw_lr 0\t\n",
            "Acc: 0.8669736842105263\n",
            "\n",
            "\n",
            "Recall 0.8669736842105263\n",
            "F1 score 0.8669736842105263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YRNLx1hjg-iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tqdm import tqdm\n",
        "from openprompt.data_utils.text_classification_dataset import AgnewsProcessor, DBpediaProcessor, ImdbProcessor, AmazonProcessor\n",
        "from openprompt.data_utils.huggingface_dataset import YahooAnswersTopicsProcessor\n",
        "import torch\n",
        "from openprompt.data_utils.utils import InputExample\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from openprompt import PromptDataLoader\n",
        "from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer, SoftVerbalizer, AutomaticVerbalizer\n",
        "from openprompt.prompts import ManualTemplate\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"\")\n",
        "\n",
        "parser.add_argument(\"--model\", type=str, default='roberta')\n",
        "parser.add_argument(\"--model_name_or_path\", default='roberta-large')\n",
        "parser.add_argument(\"--result_file\", type=str, default=\"sfs_scripts/results_fewshot_manual_kpt.txt\")\n",
        "parser.add_argument(\"--openprompt_path\", type=str, default=\"datasets\")\n",
        "\n",
        "parser.add_argument(\"--shot\", type=int, default=5)\n",
        "parser.add_argument(\"--seed\", type=int, default=144)\n",
        "parser.add_argument(\"--plm_eval_mode\",default=True, action=\"store_true\")\n",
        "parser.add_argument(\"--verbalizer\",default='kpt', type=str)\n",
        "parser.add_argument(\"--calibration\",default=True, action=\"store_true\")\n",
        "parser.add_argument(\"--filter\", default=\"tfidf_filter\", type=str)\n",
        "parser.add_argument(\"--template_id\", default=0,type=int)\n",
        "parser.add_argument(\"--dataset\",default='agnews',type=str)\n",
        "\n",
        "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
        "parser.add_argument(\"--max_epochs\", type=int, default=15)\n",
        "parser.add_argument(\"--kptw_lr\", default=0, type=float)\n",
        "parser.add_argument(\"--pred_temp\", default=1.0, type=float)\n",
        "parser.add_argument(\"--max_token_split\", default=-1, type=int)\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "import random\n",
        "this_run_unicode = str(random.randint(0, 1e10))\n",
        "\n",
        "from openprompt.utils.reproduciblity import set_seed\n",
        "set_seed(args.seed)\n",
        "\n",
        "from openprompt.plms import load_plm\n",
        "plm, tokenizer, model_config, WrapperClass = load_plm(args.model, args.model_name_or_path)\n",
        "\n",
        "dataset = {}\n",
        "\n",
        "if args.dataset == \"agnews\":\n",
        "    dataset['train'] = AgnewsProcessor().get_train_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    dataset['test'] = AgnewsProcessor().get_test_examples(f\"{args.openprompt_path}/TextClassification/agnews/\")\n",
        "    class_labels =AgnewsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/agnews\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 10\n",
        "elif args.dataset == \"dbpedia\":\n",
        "    dataset['train'] = DBpediaProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    dataset['test'] = DBpediaProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/dbpedia/\")\n",
        "    class_labels =DBpediaProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/dbpedia\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"yahoo\":\n",
        "    dataset['train'] = YahooAnswersTopicsProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    dataset['test'] = YahooAnswersTopicsProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/yahoo_answers_topics/\")\n",
        "    class_labels =YahooAnswersTopicsProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/yahoo_answers_topics\"\n",
        "    scriptformat = \"json\"\n",
        "    cutoff=0.5\n",
        "    max_seq_l = 128\n",
        "    batch_s = 30\n",
        "elif args.dataset == \"imdb\":\n",
        "    dataset['train'] = ImdbProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    dataset['test'] = ImdbProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/imdb/\")\n",
        "    class_labels = ImdbProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/imdb\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "elif args.dataset == \"amazon\":\n",
        "    dataset['train'] = AmazonProcessor().get_train_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    dataset['test'] = AmazonProcessor().get_test_examples(f\"{args.openprompt_path}/datasets/TextClassification/amazon/\")\n",
        "    class_labels = AmazonProcessor().get_labels()\n",
        "    scriptsbase = \"TextClassification/amazon\"\n",
        "    scriptformat = \"txt\"\n",
        "    cutoff=0\n",
        "    max_seq_l = 512\n",
        "    batch_s = 5\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_template.txt\", choice=args.template_id)\n",
        "\n",
        "\n",
        "if args.verbalizer == \"kpt\":\n",
        "    myverbalizer = KnowledgeableVerbalizer(tokenizer, classes=class_labels, candidate_frac=cutoff, pred_temp=args.pred_temp, max_token_split=args.max_token_split).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/knowledgeable_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"manual\":\n",
        "    myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"soft\":\n",
        "    myverbalizer = SoftVerbalizer(tokenizer, model=plm, classes=class_labels).from_file(f\"{args.openprompt_path}/scripts/{scriptsbase}/manual_verbalizer.{scriptformat}\")\n",
        "elif args.verbalizer == \"auto\":\n",
        "    myverbalizer = AutomaticVerbalizer(tokenizer, classes=class_labels)\n",
        "\n",
        "\n",
        "# (contextual) calibration\n",
        "if args.verbalizer in [\"kpt\",\"manual\"]:\n",
        "    if args.calibration or args.filter != \"none\":\n",
        "        from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "        support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)\n",
        "        dataset['support'] = support_sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "        # for example in dataset['support']:\n",
        "        #     example.label = -1 # remove the labels of support set for clarification\n",
        "        support_dataloader = PromptDataLoader(dataset=dataset[\"support\"], template=mytemplate, tokenizer=tokenizer,\n",
        "            tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "            batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "            truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "from openprompt import PromptForClassification\n",
        "use_cuda = True\n",
        "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False, plm_eval_mode=args.plm_eval_mode)\n",
        "if use_cuda:\n",
        "    prompt_model=  prompt_model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "# HP\n",
        "# if args.calibration:\n",
        "if args.verbalizer in [\"kpt\",\"manual\"]:\n",
        "    if args.calibration or args.filter != \"none\":\n",
        "        org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "        from contextualize_calibration import calibrate\n",
        "        # calculate the calibration logits\n",
        "        cc_logits = calibrate(prompt_model, support_dataloader)\n",
        "        print(\"the calibration logits is\", cc_logits)\n",
        "        print(\"origial label words num {}\".format(org_label_words_num))\n",
        "\n",
        "    if args.calibration:\n",
        "        myverbalizer.register_calibrate_logits(cc_logits.mean(dim=0))\n",
        "        new_label_words_num = [len(myverbalizer.label_words[i]) for i in range(len(class_labels))]\n",
        "        print(\"After filtering, number of label words per class: {}\".format(new_label_words_num))\n",
        "\n",
        "\n",
        "    from filter_method import *\n",
        "    if args.filter == \"tfidf_filter\":\n",
        "        tfidf_filter(myverbalizer, cc_logits, class_labels)\n",
        "    elif args.filter == \"none\":\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    # register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits\n",
        "    # currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.\n",
        "\n",
        "from openprompt.data_utils.data_sampler import FewShotSampler\n",
        "sampler = FewShotSampler(num_examples_per_label=args.shot, also_sample_dev=True, num_examples_per_label_dev=args.shot)\n",
        "dataset['train'], dataset['validation'] = sampler(dataset['train'], seed=args.seed)\n",
        "\n",
        "\n",
        "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "# zero-shot test\n",
        "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
        "    tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
        "    batch_size=batch_s,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
        "    truncate_method=\"tail\")\n",
        "\n",
        "\n",
        "def evaluate(prompt_model, dataloader, desc):\n",
        "    prompt_model.eval()\n",
        "    allpreds = []\n",
        "    alllabels = []\n",
        "    pbar = tqdm(dataloader, desc=desc)\n",
        "    for step, inputs in enumerate(pbar):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)\n",
        "        labels = inputs['label']\n",
        "        alllabels.extend(labels.cpu().tolist())\n",
        "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
        "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
        "\n",
        "    recall=sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(alllabels)\n",
        "    micro_f1=2*acc*recall/(acc+recall)\n",
        "\n",
        "    return acc,recall,micro_f1\n",
        "############\n",
        "#############\n",
        "###############\n",
        "\n",
        "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def prompt_initialize(verbalizer, prompt_model, init_dataloader):\n",
        "    dataloader = init_dataloader\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Init_using_{}\".format(\"train\")):\n",
        "            batch = batch.cuda()\n",
        "            logits = prompt_model(batch)\n",
        "        verbalizer.optimize_to_initialize()\n",
        "'''\n",
        "This is an epoch-level optimize. If used in batch-level like an ordinary gradient descend optimizer,\n",
        " the result may not be very satisfying since the accumated examples (i.e., the probs_buffer and the labels_buffer) are not enough if the batchsize is small.'''\n",
        "\n",
        "if args.verbalizer == \"soft\":\n",
        "\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer_grouped_parameters2 = [\n",
        "        {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
        "        {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4},\n",
        "    ]\n",
        "\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "    optimizer2 = AdamW(optimizer_grouped_parameters2)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    scheduler2 = get_linear_schedule_with_warmup(\n",
        "        optimizer2,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "elif args.verbalizer == \"auto\":\n",
        "    prompt_initialize(myverbalizer, prompt_model, train_dataloader)\n",
        "\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    optimizer2 = None\n",
        "    scheduler2 = None\n",
        "\n",
        "elif args.verbalizer == \"kpt\":\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    # optimizer_grouped_parameters2 = [\n",
        "    #     {'params': , \"lr\":1e-1},\n",
        "    # ]\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "    optimizer2 = AdamW(prompt_model.verbalizer.parameters(), lr=args.kptw_lr)\n",
        "    # print(optimizer_grouped_parameters2)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    # scheduler2 = get_linear_schedule_with_warmup(\n",
        "    #     optimizer2,\n",
        "    #     num_warmup_steps=0, num_training_steps=tot_step)\n",
        "    scheduler2 = None\n",
        "\n",
        "elif args.verbalizer == \"manual\":\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
        "    optimizer_grouped_parameters1 = [\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    # Using different optimizer for prompt parameters and model parameters\n",
        "\n",
        "    optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
        "\n",
        "    tot_step = len(train_dataloader) // args.gradient_accumulation_steps * args.max_epochs\n",
        "    scheduler1 = get_linear_schedule_with_warmup(\n",
        "        optimizer1,\n",
        "        num_warmup_steps=0, num_training_steps=tot_step)\n",
        "\n",
        "    optimizer2 = None\n",
        "    scheduler2 = None\n",
        "\n",
        "\n",
        "tot_loss = 0\n",
        "log_loss = 0\n",
        "best_val_acc = 0\n",
        "for epoch in range(args.max_epochs):\n",
        "    tot_loss = 0\n",
        "    prompt_model.train()\n",
        "    for step, inputs in enumerate(train_dataloader):\n",
        "        if use_cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        logits = prompt_model(inputs)#bs*class_nums\n",
        "        labels = inputs['label']#bs*1\n",
        "        loss = loss_func(logits, labels)\n",
        "        loss.requires_grad_(True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
        "        tot_loss += loss.item()\n",
        "        optimizer1.step()\n",
        "        scheduler1.step()\n",
        "        optimizer1.zero_grad()\n",
        "        if optimizer2 is not None:\n",
        "            optimizer2.step()\n",
        "            optimizer2.zero_grad()\n",
        "        if scheduler2 is not None:\n",
        "            scheduler2.step()\n",
        "    \n",
        "    val_acc,r,f = evaluate(prompt_model, validation_dataloader, desc=\"Valid\")\n",
        "    if val_acc>=best_val_acc:\n",
        "        torch.save(prompt_model.state_dict(),f\"ckpts/{this_run_unicode}.ckpt\")\n",
        "        best_val_acc = val_acc\n",
        "    print(\"Epoch {},train_loss  {} val_acc {}\".format(epoch,tot_loss/len(train_dataloader),val_acc), flush=True)\n",
        "\n",
        "prompt_model.load_state_dict(torch.load(f\"ckpts/{this_run_unicode}.ckpt\"))\n",
        "prompt_model = prompt_model.cuda()\n",
        "test_acc,test_r,test_f = evaluate(prompt_model, test_dataloader, desc=\"Test\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "content_write = \"=\"*20+\"\\n\"\n",
        "content_write += f\"dataset {args.dataset}\\t\"\n",
        "content_write += f\"temp {args.template_id}\\t\"\n",
        "content_write += f\"seed {args.seed}\\t\"\n",
        "content_write += f\"shot {args.shot}\\t\"\n",
        "content_write += f\"verb {args.verbalizer}\\t\"\n",
        "content_write += f\"cali {args.calibration}\\t\"\n",
        "content_write += f\"filt {args.filter}\\t\"\n",
        "content_write += f\"maxsplit {args.max_token_split}\\t\"\n",
        "content_write += f\"kptw_lr {args.kptw_lr}\\t\"\n",
        "content_write += \"\\n\"\n",
        "content_write += f\"Acc: {test_acc}\"\n",
        "content_write += \"\\n\\n\"\n",
        "\n",
        "print(content_write)\n",
        "print('Recall',test_r)\n",
        "print('F1 score',test_f)\n",
        "\n",
        "with open(f\"{args.result_file}\", \"a\") as fout:\n",
        "    fout.write(content_write)\n",
        "\n",
        "import os\n",
        "os.remove(f\"ckpts/{this_run_unicode}.ckpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73ec2dd-992c-41da-ac37-61a7fb79e605",
        "id": "--wt27hEg-1m"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##Num of label words for each label: [376, 350, 287, 366]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 200it [00:00, 461.23it/s]\n",
            "ContextCali: 100%|██████████| 20/20 [00:02<00:00,  6.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the calibration logits is tensor([[33.6367, -4.4846, 45.7015,  ..., -0.3739, -0.1965, 25.9612],\n",
            "        [31.4200, -3.7351, 44.5797,  ...,  0.7121,  1.0803, 25.0002],\n",
            "        [34.7010, -4.0117, 46.3250,  ...,  0.3138,  1.7178, 26.9313],\n",
            "        ...,\n",
            "        [32.8307, -4.9485, 44.5945,  ..., -0.7820, -0.3619, 25.7531],\n",
            "        [35.0306, -4.8419, 45.5432,  ..., -0.3878,  0.4692, 27.0337],\n",
            "        [32.5885, -4.1696, 46.6274,  ...,  0.5883,  1.3667, 25.0157]],\n",
            "       device='cuda:0')\n",
            "origial label words num [376, 350, 287, 366]\n",
            "##Num of label words for each label: [228, 269, 231, 245]\n",
            "After filtering, number of label words per class: [228, 269, 231, 245]\n",
            "norm_ord 5.975124378109453\n",
            "optimal_cut rate is 0.6579925650557621\n",
            "##Num of label words for each label: [150, 177, 151, 161]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "tokenizing: 20it [00:00, 462.74it/s]\n",
            "tokenizing: 20it [00:00, 416.78it/s]\n",
            "tokenizing: 7600it [00:12, 608.99it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0,train_loss  0.5654636472463608 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1,train_loss  0.5654636770486832 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2,train_loss  0.5654636919498444 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3,train_loss  0.565463662147522 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4,train_loss  0.5654637217521667 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5,train_loss  0.5654636919498444 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6,train_loss  0.5654636323451996 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7,train_loss  0.5654637217521667 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8,train_loss  0.5654636472463608 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9,train_loss  0.5654636919498444 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10,train_loss  0.5654636770486832 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11,train_loss  0.5654636919498444 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12,train_loss  0.5654637068510056 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13,train_loss  0.565463662147522 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 2/2 [00:00<00:00,  6.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14,train_loss  0.565463662147522 val_acc 0.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 760/760 [01:52<00:00,  6.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "dataset agnews\ttemp 0\tseed 144\tshot 5\tverb kpt\tcali True\tfilt tfidf_filter\tmaxsplit -1\tkptw_lr 0\t\n",
            "Acc: 0.8669736842105263\n",
            "\n",
            "\n",
            "Recall 0.8669736842105263\n",
            "F1 score 0.8669736842105263\n"
          ]
        }
      ]
    }
  ]
}